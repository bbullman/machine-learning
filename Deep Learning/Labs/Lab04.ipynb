{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "rd6I7xv62sA9"
      },
      "source": [
        "# TensorFlow Convolutional Neural Network for Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "FQUPvLQb2sA_"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "NFLhPj1A2sBB"
      },
      "source": [
        "## Imports\n",
        "Here, we import keras, the model we are going to use (a sequential model), three different types of layers namely convolution layer, max-pooling layer and dense layers. We will also import activation functions and flattening operation from the layers module."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Activation, Flatten, Dropout\n",
        "from keras.optimizers import RMSprop"
      ],
      "metadata": {
        "id": "j8JBEAda8Vir"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "SWX6gwHo2sBB"
      },
      "source": [
        "## Load Data\n",
        "There is a lot to building a succesful model. Loading datasets, formatting them, presenting them to the model in batches are all part of a successul machine learning algorithm. Here we show how the specific data we are using is loaded, and formatted in the Keras library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "vFHJgr372sBB"
      },
      "outputs": [],
      "source": [
        "#from keras.datasets.cifar import load_batch #cifar doesn't exist in Keras Python 3\n",
        "from keras.datasets.cifar10 import load_data as load_data_cifar10\n",
        "from keras import backend as K\n",
        "\n",
        "# Additional helper function cobbled together from scouring the internet\n",
        "import pickle\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = pickle.load(f, encoding='bytes')\n",
        "    d_decoded = {}\n",
        "    for k, v in datadict.items():\n",
        "        d_decoded[k.decode('utf8')] = v\n",
        "    datadict = d_decoded\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0, 1, 2, 3).astype('float')\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads CIFAR10 dataset.\n",
        "    # Returns\n",
        "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
        "    \"\"\"\n",
        "    path = './cifar-10-batches-py'\n",
        "\n",
        "    num_train_samples = 50000\n",
        "\n",
        "    x_train = np.empty((num_train_samples, 3, 32, 32), dtype='uint8')\n",
        "    y_train = np.empty((num_train_samples,), dtype='uint8')\n",
        "\n",
        "    for i in range(1, 6):\n",
        "        fpath = os.path.join(path, 'data_batch_' + str(i))\n",
        "        (x_train[(i - 1) * 10000: i * 10000, :, :, :],\n",
        "         y_train[(i - 1) * 10000: i * 10000]) = load_CIFAR_batch(fpath) # load_batch(fpath) load_batch doesn't exist in keras.dataset.cifar10\n",
        "\n",
        "    fpath = os.path.join(path, 'test_batch')\n",
        "    x_test, y_test = load_CIFAR_batch(fpath) # load_Batch(fpath)\n",
        "\n",
        "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
        "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
        "\n",
        "    if K.image_data_format() == 'channels_last':\n",
        "        x_train = x_train.transpose(0, 2, 3, 1)\n",
        "        x_test = x_test.transpose(0, 2, 3, 1)\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "dmeY-cHq2sBC"
      },
      "source": [
        "## Exploring the dataset\n",
        "First we need to figure out our image size and number of categories(or classes) we are trying model. Fill in the following parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "id": "hu0UZ3oj2sBC"
      },
      "outputs": [],
      "source": [
        "#  The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "def params():\n",
        "    num_classes = 10\n",
        "    in_shape = (32,32,3)\n",
        "    batch_size = 32\n",
        "    epochs = 10\n",
        "    return [num_classes, in_shape, batch_size, epochs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "esbk6iuc2sBC"
      },
      "outputs": [],
      "source": [
        "[num_classes, in_shape, batch_size, epochs] = params()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "CCYXGZpV2sBD"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'keras_cifar10_trained_model.h5'\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "(x_train, y_train), (x_test, y_test) = load_data()\n",
        "\n",
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yS82PjNd2sBD"
      },
      "source": [
        "## Building the Model\n",
        "Here you are going to build the CNN model. Provide the implementation for ConvConvMaxpoolDropout followed by ConvConvMaxpoolDropout and a classification module of DenseDropDense layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Three different types of layers namely convolution layer (Conv2D), max-pooling layer, and dense layers. We also use the flattening, activation and dropout\n",
        "# I see now the ask above is saying to do a Conv2d - Conv2d - MaxPool - Dropout set layers + regularization, and then another set for the second one, then a third one with flatten, dense, dropout dense\n",
        "\n",
        "def build_model(in_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    # ConvConvMaxpoolDropout\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same', input_shape=in_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.2)) # Drop out 20% of the nodes\n",
        "\n",
        "    # ConvConvMaxpoolDropout with more filters (I just doubled the input)\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.3)) # Drop out 30% of the nodes\n",
        "\n",
        "    # DenseDropDense\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=128, activation='relu')) # units equal to previous layer (64+64)\n",
        "    model.add(Dropout(0.5)) # Dropout 50% of the nodes\n",
        "    model.add(Dense(units=10, activation='softmax')) # 10 output classes, use softmax function for activation\n",
        "    return model"
      ],
      "metadata": {
        "id": "x3o74KuAL5oo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Putting everything together. Build your model, initiate RMSprop optimizer, and compile it with categorical_crossentropy\n",
        "def train_cifar():\n",
        "    model = build_model(in_shape)\n",
        "    opt = RMSprop(learning_rate=0.0001, rho=0.9, momentum=0.0) #weight_decay=1e-6)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, batch_size=32, epochs=25, validation_data=(x_test, y_test), shuffle=True)\n",
        "    return model\n",
        "\n",
        "model = train_cifar()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvx8YwLYIv9X",
        "outputId": "089ce941-cf33-48cb-805f-8cdce739f1e9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 180ms/step - accuracy: 0.2248 - loss: 2.0914 - val_accuracy: 0.4311 - val_loss: 1.6005\n",
            "Epoch 2/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 182ms/step - accuracy: 0.4051 - loss: 1.6437 - val_accuracy: 0.4957 - val_loss: 1.4004\n",
            "Epoch 3/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m327s\u001b[0m 185ms/step - accuracy: 0.4657 - loss: 1.4812 - val_accuracy: 0.5352 - val_loss: 1.2914\n",
            "Epoch 4/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 182ms/step - accuracy: 0.5091 - loss: 1.3724 - val_accuracy: 0.5769 - val_loss: 1.2072\n",
            "Epoch 5/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 184ms/step - accuracy: 0.5384 - loss: 1.2959 - val_accuracy: 0.6009 - val_loss: 1.1476\n",
            "Epoch 6/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 184ms/step - accuracy: 0.5680 - loss: 1.2267 - val_accuracy: 0.6021 - val_loss: 1.1118\n",
            "Epoch 7/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 182ms/step - accuracy: 0.5910 - loss: 1.1611 - val_accuracy: 0.6428 - val_loss: 1.0192\n",
            "Epoch 8/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 180ms/step - accuracy: 0.6185 - loss: 1.1003 - val_accuracy: 0.6483 - val_loss: 0.9942\n",
            "Epoch 9/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 182ms/step - accuracy: 0.6284 - loss: 1.0570 - val_accuracy: 0.6747 - val_loss: 0.9328\n",
            "Epoch 10/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 179ms/step - accuracy: 0.6375 - loss: 1.0296 - val_accuracy: 0.6590 - val_loss: 0.9632\n",
            "Epoch 11/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 176ms/step - accuracy: 0.6546 - loss: 0.9885 - val_accuracy: 0.6846 - val_loss: 0.8993\n",
            "Epoch 12/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 183ms/step - accuracy: 0.6632 - loss: 0.9653 - val_accuracy: 0.6966 - val_loss: 0.8721\n",
            "Epoch 13/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 185ms/step - accuracy: 0.6739 - loss: 0.9431 - val_accuracy: 0.7125 - val_loss: 0.8302\n",
            "Epoch 14/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 187ms/step - accuracy: 0.6799 - loss: 0.9224 - val_accuracy: 0.7104 - val_loss: 0.8485\n",
            "Epoch 15/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m332s\u001b[0m 194ms/step - accuracy: 0.6913 - loss: 0.8956 - val_accuracy: 0.7164 - val_loss: 0.8331\n",
            "Epoch 16/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 187ms/step - accuracy: 0.6918 - loss: 0.8940 - val_accuracy: 0.7059 - val_loss: 0.8393\n",
            "Epoch 17/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 189ms/step - accuracy: 0.6993 - loss: 0.8840 - val_accuracy: 0.7224 - val_loss: 0.8181\n",
            "Epoch 18/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 188ms/step - accuracy: 0.7058 - loss: 0.8678 - val_accuracy: 0.7364 - val_loss: 0.7747\n",
            "Epoch 19/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 186ms/step - accuracy: 0.7084 - loss: 0.8575 - val_accuracy: 0.7275 - val_loss: 0.8087\n",
            "Epoch 20/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 187ms/step - accuracy: 0.7116 - loss: 0.8425 - val_accuracy: 0.7385 - val_loss: 0.8007\n",
            "Epoch 21/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 186ms/step - accuracy: 0.7166 - loss: 0.8409 - val_accuracy: 0.7386 - val_loss: 0.7728\n",
            "Epoch 22/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 186ms/step - accuracy: 0.7171 - loss: 0.8250 - val_accuracy: 0.7408 - val_loss: 0.7662\n",
            "Epoch 23/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 185ms/step - accuracy: 0.7238 - loss: 0.8243 - val_accuracy: 0.7300 - val_loss: 0.7930\n",
            "Epoch 24/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 187ms/step - accuracy: 0.7237 - loss: 0.8117 - val_accuracy: 0.7495 - val_loss: 0.7273\n",
            "Epoch 25/25\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 189ms/step - accuracy: 0.7272 - loss: 0.8039 - val_accuracy: 0.7426 - val_loss: 0.7591\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5BjxEVJJ2sBD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ecc8ab-e835-436f-e4d1-d74d426ada63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved trained model at /content/saved_models/keras_cifar10_trained_model.h5 \n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 48ms/step - accuracy: 0.7437 - loss: 0.7548\n",
            "Test loss: 0.759059727191925\n",
            "Test accuracy: 0.7426000237464905\n"
          ]
        }
      ],
      "source": [
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "1. The model is structured above with multiple Convolutional (Conv2D) layers of filters, two at each \"section\", with ReLU activations with dropout, followed by flattening, adding a dense layer, and dropout. There's a total of 4 convolutional layers with max pooling done between every set of 2, and then a dense layer at the end with 128 units (I initially thought I was connecting to the 64+64 of the previous layer). Same padding keeps the same shape between Convolutional layers, but with that I'm not sure how projecting the 3x3 into the greater filter space of the next layer, then replacing every 2x2 area with it's max value really works out to a good number of filters in the next layer. I just took a naive approach to the calculations and decided to double the values between the \"sections\" of the question.\n",
        "2. I tried several different values for our main hyperparameters such as learning_rate, weight_decay, etc. and found that 0.0001 gave significantly better results than 0.01 and 0.00001 over 10 epochs. I extrapolated this to 25 epochs (which took quite awhile to run) and saw increasing accuracy. As accuracy began to increase, the error term started flattening and I gained less accuracy over time as expected. A 72.72% accuracy isn't terrible though for this sort of model, although probably not perfect."
      ],
      "metadata": {
        "id": "QSgqgeCkMWGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4sI6zMbd2sBD"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "\n",
        "jupyter nbconvert --to html /content/Lab04.ipynb"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3 [3.6]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}