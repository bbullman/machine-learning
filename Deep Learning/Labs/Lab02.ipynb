{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "9p5OhT54Yu7b"
      },
      "source": [
        "# Lesson 2: Neural Net Computations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "ZFuyQBM9Yu7c"
      },
      "source": [
        "In the previous assignment we used Keras to train a neural network. In this assignment you will build your own minimal neural net library. The basic structure is given to you; you will need to fill in details such as weight updating for backpropogation. Then you will test the network on learning the XOR function.\n",
        "\n",
        "Read through the class definitions below first to understand the basic architecture.\n",
        "\n",
        "Then you should add code as necessary where marked \"TODO\" in the code below and remove the NotImplementedError exceptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 849,
      "metadata": {
        "collapsed": true,
        "id": "Mn_LMZ93Yu7c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "FpXBpM90Yu7d"
      },
      "source": [
        "## Define a Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 850,
      "metadata": {
        "collapsed": true,
        "id": "p8Q2tEVfYu7d"
      },
      "outputs": [],
      "source": [
        "class NNet():\n",
        "    \"\"\"Implements a basic feedforward neural network.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._layers = []  # An ordered list of layers. The first layer is the input; the final is the output.\n",
        "\n",
        "    def _add_layer(self, layer):\n",
        "        if self._layers:\n",
        "            # Update pointers. We keep a doubly-linked-list of layers for convenience.\n",
        "            prev_layer = self._layers[-1]\n",
        "            prev_layer.set_next_layer(layer)\n",
        "            layer.set_prev_layer(prev_layer)\n",
        "\n",
        "        self._layers.append(layer)\n",
        "\n",
        "    def add_input_layer(self, size, **kwargs):\n",
        "        assert type(size).__name__ == 'int', ('Input layer requires integer size. Type was %s instead.'\n",
        "                                              % type(size).__name__)\n",
        "        layer = InputLayer(size=size, **kwargs)\n",
        "        self._add_layer(layer)\n",
        "\n",
        "    def add_dense_layer(self, size, **kwargs):\n",
        "        assert type(size).__name__ == 'int', ('Dense layer requires integer size. Type was %s instead.'\n",
        "                                              % type(size).__name__)\n",
        "        # Find the previous layer's size.\n",
        "        prev_size = self._layers[-1].size()\n",
        "        layer = DenseLayer(shape=(prev_size, size), **kwargs)\n",
        "        self._add_layer(layer)\n",
        "\n",
        "    def summary(self, verbose=False):\n",
        "        \"\"\"Prints a description of the model.\"\"\"\n",
        "        for i, layer in enumerate(self._layers):\n",
        "            print('%d: %s' % (i, str(layer)))\n",
        "            if verbose:\n",
        "                print('weights:', layer.get_weights())\n",
        "                if layer._use_bias:\n",
        "                    print('bias:', layer._bias)\n",
        "                print()\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Given an input vector x, run it through the neural network and return the output vector.\"\"\"\n",
        "        assert isinstance(x, np.ndarray)\n",
        "\n",
        "        # Ensure the shape of the input data is good since feed forward likes to fail on assertions.\n",
        "        if x.ndim == 1:\n",
        "          x = x.reshape(1, -1)\n",
        "\n",
        "        input_layer = self._layers[0]\n",
        "        assert x.shape[1] == input_layer.size(), \"Input shape does not match the input size\"\n",
        "\n",
        "        for layer in self._layers:\n",
        "          x = layer.feed_forward(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def train_single_example(self, X_data, y_data, learning_rate=0.01, verbose=False): # Added verbose\n",
        "        \"\"\"Train on a single example. X_data and y_data must be numpy arrays.\"\"\"\n",
        "\n",
        "        assert isinstance(X_data, np.ndarray)\n",
        "        assert isinstance(y_data, np.ndarray)\n",
        "\n",
        "        # Forward propagation. Can use predict. Need to ensure the data is the right shape just in case.\n",
        "        X_data = X_data.reshape(1, -1)\n",
        "        output = self.predict(X_data)\n",
        "\n",
        "        # Do the output layer backpropagation\n",
        "        error = output - y_data\n",
        "        error = self._layers[-1].backpropagate(error, learning_rate)\n",
        "        if verbose:\n",
        "          print(f'{self._layers[-1]._name} output error: {error}')\n",
        "\n",
        "        # Backpropagation for the rest of the layers\n",
        "        #   Skip the first one because it's the output layer that we just did above\n",
        "        for layer in reversed(self._layers[:-1]):\n",
        "          error = layer.backpropagate(error, learning_rate)\n",
        "          if verbose:\n",
        "            print(f'{layer._name} error: {error}')\n",
        "\n",
        "    def train(self, X_data, y_data, learning_rate, num_epochs, randomize=True, verbose=True, print_every_n=100):\n",
        "        \"\"\"Both X_data and y_data should be ndarrays. One example per row.\n",
        "\n",
        "        This function takes the data and learning rate, and trains the network for num_epochs passes over the\n",
        "        complete data set.\n",
        "\n",
        "        If randomize==True, the X_data and y_data should be randomized at the start of each epoch. Of course,\n",
        "        matching X,y pairs should have matching indices after randomization, to avoid scrambling the dataset.\n",
        "        (E.g., a set of indices should be randomized once and then applied to both X and y data.)\n",
        "\n",
        "        If verbose==True, will print a status report every print_every_n epochs with these\n",
        "        results:\n",
        "\n",
        "        * Results of running \"predict\" on each example in the training set\n",
        "        * MSE (mean squared error) on the dataset\n",
        "        * Accuracy on the dataset\n",
        "        \"\"\"\n",
        "        assert isinstance(X_data, np.ndarray)\n",
        "        assert isinstance(y_data, np.ndarray)\n",
        "        assert X_data.shape[0] == y_data.shape[0]\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "          if randomize:\n",
        "            indices = np.arange(X_data.shape[0])\n",
        "            np.random.shuffle(indices)\n",
        "            X_data = X_data[indices]\n",
        "            y_data = y_data[indices]\n",
        "\n",
        "          for i in range(X_data.shape[0]):\n",
        "            self.train_single_example(X_data[i], y_data[i], learning_rate, verbose=False)\n",
        "\n",
        "          if verbose and (epoch % print_every_n == 0):\n",
        "            pred = self.predict(X_data)\n",
        "            mse = self.compute_mean_squared_error(X_data, y_data)\n",
        "            acc = self.compute_accuracy(X_data, y_data)\n",
        "            print(f'Epoch {epoch}: Prediction {pred}\\n MSE = {mse:.4f}, Accuracy = {acc:.4f}')\n",
        "\n",
        "    def compute_mean_squared_error(self, X_data, y_data):\n",
        "        \"\"\"Given input X_data and target y_data, compute and return the mean squared error.\"\"\"\n",
        "        assert isinstance(X_data, np.ndarray)\n",
        "        assert isinstance(y_data, np.ndarray)\n",
        "        assert X_data.shape[0] == y_data.shape[0]\n",
        "\n",
        "        predictions = self.predict(X_data)\n",
        "        mse = np.mean(np.square(predictions - y_data))\n",
        "        return mse\n",
        "\n",
        "    def compute_accuracy(self, X_data, y_data):\n",
        "        \"\"\"Given input X_data and target y_data, convert outputs to binary using a threshold of 0.5\n",
        "        and return the accuracy: # examples correct / total # examples.\"\"\"\n",
        "        assert isinstance(X_data, np.ndarray)\n",
        "        assert isinstance(y_data, np.ndarray)\n",
        "        assert X_data.shape[0] == y_data.shape[0]\n",
        "\n",
        "        correct = 0\n",
        "        for i in range(len(X_data)):\n",
        "            outputs = self.predict(X_data[i])\n",
        "            outputs = outputs > 0.5\n",
        "            if outputs == y_data[i]:\n",
        "                correct += 1\n",
        "        acc = float(correct) / len(X_data)\n",
        "        return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "cAVHIh6-Yu7d"
      },
      "source": [
        "## Define activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 851,
      "metadata": {
        "collapsed": true,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "nNNfdb6eYu7e"
      },
      "outputs": [],
      "source": [
        "class Activation():  # Do not edit; update derived classes.\n",
        "    \"\"\"Base class that represents an activation function and knows how to take its own derivative.\"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "\n",
        "    def activate(x):\n",
        "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def derivative_given_y(self, y):\n",
        "        \"\"\"y is a scalar or a numpy array.\n",
        "\n",
        "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
        "        raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 852,
      "metadata": {
        "collapsed": true,
        "id": "-Nxlnh-rYu7e"
      },
      "outputs": [],
      "source": [
        "class IdentityActivation(Activation):\n",
        "    \"\"\"Activation function that passes input through unchanged.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(name='Identity')\n",
        "\n",
        "    def activate(self, x):\n",
        "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
        "        return x\n",
        "\n",
        "    def derivative_given_y(self, y):\n",
        "        \"\"\"y is a scalar or a numpy array.\n",
        "\n",
        "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
        "        return 1\n",
        "\n",
        "\n",
        "class SigmoidActivation(Activation):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(name='Sigmoid')\n",
        "\n",
        "    def activate(self, x):\n",
        "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "    def derivative_given_y(self, y):\n",
        "        \"\"\"y is a scalar or a numpy array.\n",
        "\n",
        "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
        "        return y * (1 - y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "cPCC8L9KYu7e"
      },
      "source": [
        "## Define a method to initialize neural net weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 853,
      "metadata": {
        "collapsed": true,
        "id": "u0kAoTR7Yu7e"
      },
      "outputs": [],
      "source": [
        "def WeightInitializer():\n",
        "    \"\"\"Function to return a random weight. for example, return a random float from -1 to 1.\"\"\"\n",
        "    return round(random.uniform(-1.0,1.0), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "GH57uHiRYu7e"
      },
      "source": [
        "## Define a neural net Layer base class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 854,
      "metadata": {
        "collapsed": true,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "_KLV9LADYu7e"
      },
      "outputs": [],
      "source": [
        "class Layer():\n",
        "    \"\"\"Base class for NNet layers. DO NOT MODIFY THIS CLASS. Update derived classes instead.\n",
        "\n",
        "    Conceptually, in this library a Layer consists at a high level of:\n",
        "      * a collection of weights (a 2D numpy array)\n",
        "      * the output nodes that come after the weights above\n",
        "      * the activation function that is applied to the summed signals in these output nodes\n",
        "\n",
        "    So a Layer isn't just nodes -- it's weights as well as nodes.\n",
        "\n",
        "    Specifically, to send signal forward through a 3-layer network, we start with an Input Layer that does\n",
        "    very little.  The outputs from the Input layer are simply the fed-in input data.\n",
        "\n",
        "    Then, the next layer will be a Dense layer that holds the weights from the Input layer to the first hidden\n",
        "    layer and stores the activation function to be used after doing a product of weights and Input-Layer\n",
        "    outputs.\n",
        "\n",
        "    Finally, another Dense layer will hold the weights from the hidden to the output layer nodes, and stores\n",
        "    the activation function to be applied to the final output nodes.\n",
        "\n",
        "    For a typical 1-hidden layer network, then, we would have 1 Input layer and 2 Dense layers.\n",
        "\n",
        "    Each Layer also has funcitons to perform the forward-pass and backpropagation steps for the weights/nodes\n",
        "    associated with the layer.\n",
        "\n",
        "    Finally, each Layer stores pointers to the pervious and next layers, for convenience when implementing\n",
        "    backprop.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, shape, use_bias, activation_function=IdentityActivation, weight_initializer=None, name=''):\n",
        "        # These are the weights from the *previous* layer to the current layer.\n",
        "        self._weights = None\n",
        "\n",
        "        # Tuple of (# inputs, # outputs) for Dense layers or just a scalar for an input layer.\n",
        "        assert type(shape).__name__ == 'int' or type(shape).__name__ == 'tuple', (\n",
        "            'shape must be scalar or a 2-element tuple')\n",
        "        if type(shape).__name__ == 'tuple':\n",
        "            assert len(shape)==2, 'shape must be 2-dimensional. Was %d instead' % len(shape)\n",
        "        self._shape = shape\n",
        "\n",
        "        # True to use a bias node that inputs to each node in this layer; False otherwise.\n",
        "        self._use_bias = use_bias\n",
        "\n",
        "        if use_bias:\n",
        "            bias_size = shape[-1] if len(shape) > 1 else shape\n",
        "            self._bias = np.zeros(bias_size)\n",
        "            if weight_initializer:\n",
        "                for i in range(bias_size):\n",
        "                    self._bias[i] = weight_initializer()\n",
        "\n",
        "        # Activation function to be applied to each dot product of weights with inputs.\n",
        "        # Instantiate an object of this class.\n",
        "        self._activation_function = activation_function() if activation_function else None\n",
        "\n",
        "        # Method used to initialize the weights in this Layer at creation time.\n",
        "        self._weight_initializer = weight_initializer\n",
        "\n",
        "        # Layer name (optional)\n",
        "        self._name = name\n",
        "\n",
        "        # Calculated output vector from the most recent feed_forward(inputs) call.\n",
        "        self._outputs = None\n",
        "\n",
        "        # Doubly linked list pointers to neighbor layers.\n",
        "        self._prev_layer = None  # Previous layer is closer to (or is) the input layer.\n",
        "        self._next_layer = None  # Next layer is closer to (or is) the output layer.\n",
        "\n",
        "    def set_prev_layer(self, layer):\n",
        "        \"\"\"Set pointer to the previous layer.\"\"\"\n",
        "        self._prev_layer = layer\n",
        "\n",
        "    def set_next_layer(self, layer):\n",
        "        \"\"\"Set pointer to the next layer.\"\"\"\n",
        "        self._next_layer = layer\n",
        "\n",
        "    def size(self):\n",
        "        \"\"\"Number of nodes in this layer.\"\"\"\n",
        "        if type(self._shape).__name__ == 'tuple':\n",
        "            return self._shape[-1]\n",
        "        else:\n",
        "            return self._shape\n",
        "\n",
        "    def get_weights(self):\n",
        "        \"\"\"Return a numpy array of the weights for inputs to this layer.\"\"\"\n",
        "        return self._weights\n",
        "\n",
        "    def get_bias(self):\n",
        "        \"\"\"Return a numpy array of the bias for nodes in this layer.\"\"\"\n",
        "        return self._bias\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        \"\"\"Feed the given inputs through the input weights and activation function, and set the outputs vector.\n",
        "\n",
        "        Also returns the outputs vector for convenience.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def backpropagate(self, error, learning_rate):\n",
        "        \"\"\"Adjusts the weights coming into this layer based on the given output error vector.\n",
        "\n",
        "        For the output layer, the \"error\" vector should be a list of output errors, y_k - t_k.\n",
        "        For a hidden layer, the \"error\" vector should be a list of the delta values from the following layer, such as delta_z_k\n",
        "\n",
        "        Returns a list of the delta values for each node in this layer. These deltas can be used as the error\n",
        "        values when calling backpropagate on the previous layer.\"\"\"\n",
        "        raise NotimplementedError()\n",
        "\n",
        "    def __str__(self):\n",
        "        activation_fxn_name = self._activation_function.name if self._activation_function else None\n",
        "        return '[%s] shape %s, use_bias=%s, activation=%s' % (self._name, self._shape, self._use_bias,\n",
        "                                                              activation_fxn_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "0bxCNmb4Yu7f"
      },
      "source": [
        "### Define InputLayer and DenseLayer base classes\n",
        "\n",
        "The DenseLayer class is where most of the computation happens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 855,
      "metadata": {
        "collapsed": true,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "jzkyeiV9Yu7f"
      },
      "outputs": [],
      "source": [
        "class InputLayer(Layer):\n",
        "    \"\"\"A neural network 1-dimensional input layer.\"\"\"\n",
        "\n",
        "    def __init__(self, size, name='Input'):\n",
        "        assert type(size).__name__ == 'int', 'Input size must be integer. Was %s instead' % type(size).__name__\n",
        "        super().__init__(shape=size, use_bias=False, name=name, activation_function=None)\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        #assert len(inputs)==self._shape, 'Inputs must be of size %d; was %d instead' % (self._shape, len(inputs)) # This throws lots of issues with tuple input\n",
        "        self._outputs = inputs\n",
        "        return self._outputs\n",
        "\n",
        "    def backpropagate(self, error, learning_rate):\n",
        "        return None  # Nothing to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 856,
      "metadata": {
        "collapsed": true,
        "id": "-mWEmQ_ZYu7f"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(Layer):\n",
        "    \"\"\"A neural network layer that is fully connected to the previous layer.\"\"\"\n",
        "\n",
        "    def __init__(self, shape, use_bias=True, name='Dense', **kwargs):\n",
        "        super().__init__(shape=shape, use_bias=use_bias, name=name, **kwargs)\n",
        "\n",
        "        self._weights = np.zeros(shape)\n",
        "        if self._weight_initializer:\n",
        "            for i in range(shape[0]):\n",
        "                for j in range(shape[1]):\n",
        "                    self._weights[i,j] = self._weight_initializer()\n",
        "\n",
        "    def feed_forward(self, inputs):\n",
        "        \"\"\"Feed the given inputs through the input weights and activation function, and set the outputs vector.\n",
        "\n",
        "        Also returns the outputs vector for convenience.\"\"\"\n",
        "        inputs = np.asarray(inputs)\n",
        "        #assert len(inputs)==self._shape, 'Inputs must be of size %d; was %d instead' % (self._shape, len(inputs)) # This throws lots of issues with tuple input\n",
        "        # Again check the dimensions and ensure 2-dimensional input in case of single input\n",
        "        if inputs.ndim == 1:\n",
        "            inputs = inputs.reshape(1, -1)\n",
        "\n",
        "        neuron = np.dot(inputs, self._weights)\n",
        "        if self._use_bias:\n",
        "          neuron += self._bias\n",
        "        output = self._activation_function.activate(neuron)\n",
        "\n",
        "        # Update output vector for later use, and return it.\n",
        "        self._outputs = output\n",
        "        return self._outputs\n",
        "\n",
        "    def backpropagate(self, error, learning_rate):\n",
        "        \"\"\"Adjusts the weights coming into this layer based on the given output error vector.\n",
        "\n",
        "        For the output layer, the \"error\" vector should be a list of output errors, y_k - t_k.\n",
        "        For a hidden layer, the \"error\" vector should be a list of the delta values from the following layer, such as delta_z_k\n",
        "\n",
        "        Returns a list of the delta values for each node in this layer. These deltas can be used as the error\n",
        "        values when calling backpropagate on the previous layer.\"\"\"\n",
        "        assert isinstance(error, np.ndarray)\n",
        "        assert isinstance(self._prev_layer._outputs, np.ndarray) # Don't we want to go backwards?\n",
        "        assert isinstance(self._outputs, np.ndarray)\n",
        "\n",
        "        # Compute deltas. Note initial error is calculated in the neural network class\n",
        "        deltas = error * self._activation_function.derivative_given_y(self._outputs)\n",
        "\n",
        "        # Compute gradient and adjust weights.\n",
        "        if(self._prev_layer is not None): # Are we in the output layer or not?\n",
        "          if self._outputs.shape[0] == 1: # Single neuron output in this network causes lots of issues, so check shape.\n",
        "            weight_update = np.dot(self._prev_layer._outputs.reshape(-1, 1), deltas.reshape(1, -1))\n",
        "          else:\n",
        "            weight_update = np.dot(self._prev_layer._outputs.T, deltas.reshape(-1, 1)) # np.newaxis more useful here?\n",
        "          self._weights -= learning_rate * weight_update\n",
        "\n",
        "        # Adjust bias weights. Check dimensions just in case.\n",
        "        if self._use_bias:\n",
        "          if deltas.ndim == 1:\n",
        "            deltas = deltas.reshape(-1, 1)\n",
        "          self._bias -= learning_rate * deltas.mean(axis=0)\n",
        "          pass\n",
        "\n",
        "        if self._next_layer and self._prev_layer is None:\n",
        "          return np.dot(self._next_layer._weights.T, deltas) # Input layer\n",
        "        return deltas # The deltas get fed in the call to backprop in the neural network class for the next layer\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "NWQxlA9uYu7f"
      },
      "source": [
        "# Train a neural net\n",
        "\n",
        "## Create a dataset for the XOR problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 857,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeYeHjDUYu7f",
        "outputId": "a66c9cd4-bd21-403c-aeb4-ab6fc3b247c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0]\n",
            " [1 0]\n",
            " [0 1]\n",
            " [1 1]]\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n"
          ]
        }
      ],
      "source": [
        "X_data = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
        "y_data = np.array([[0,1,1,0]]).T\n",
        "print(X_data)\n",
        "print(y_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "NBmUfqiWYu7f"
      },
      "source": [
        "## Create a neural network using the library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 858,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBfPlzv6Yu7f",
        "outputId": "c9eef051-c379-4d6d-92d5-06997a0e0ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: [Input] shape 2, use_bias=False, activation=None\n",
            "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
            "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n"
          ]
        }
      ],
      "source": [
        "nnet = NNet()\n",
        "nnet.add_input_layer(2)\n",
        "nnet.add_dense_layer(2, weight_initializer=WeightInitializer, activation_function=SigmoidActivation)\n",
        "nnet.add_dense_layer(1, weight_initializer=WeightInitializer, activation_function=SigmoidActivation, name='Output')\n",
        "nnet.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 859,
      "metadata": {
        "collapsed": true,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usdbyyDAYu7f",
        "outputId": "4215ed36-fd0f-453f-f60e-da5d975e0f51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: [Input] shape 2, use_bias=False, activation=None\n",
            "weights: None\n",
            "\n",
            "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
            "weights: [[ 0.85 -0.92]\n",
            " [-0.33  0.87]]\n",
            "bias: [ 0.78 -0.46]\n",
            "\n",
            "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n",
            "weights: [[ 0.56]\n",
            " [-0.32]]\n",
            "bias: [-0.77]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nnet.summary(verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "mujCiU1ZYu7g"
      },
      "source": [
        "# Train the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 860,
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzjjrYNNYu7g",
        "outputId": "ee4db725-1963-48f4-e3ba-f1086f8e4c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Prediction [[0.35012056]\n",
            " [0.40993594]\n",
            " [0.38981214]\n",
            " [0.37565345]]\n",
            " MSE = 0.2659, Accuracy = 0.5000\n",
            "Epoch 100: Prediction [[0.38755512]\n",
            " [0.41250778]\n",
            " [0.4287796 ]\n",
            " [0.44807672]]\n",
            " MSE = 0.2584, Accuracy = 0.5000\n",
            "Epoch 200: Prediction [[0.45643516]\n",
            " [0.4386385 ]\n",
            " [0.4746528 ]\n",
            " [0.41470235]]\n",
            " MSE = 0.2548, Accuracy = 0.5000\n",
            "Epoch 300: Prediction [[0.43357345]\n",
            " [0.45636418]\n",
            " [0.47522403]\n",
            " [0.49235059]]\n",
            " MSE = 0.2532, Accuracy = 0.5000\n",
            "Epoch 400: Prediction [[0.46819189]\n",
            " [0.48779585]\n",
            " [0.50389964]\n",
            " [0.44654522]]\n",
            " MSE = 0.2524, Accuracy = 0.7500\n",
            "Epoch 500: Prediction [[0.47599377]\n",
            " [0.51128734]\n",
            " [0.49612686]\n",
            " [0.45544883]]\n",
            " MSE = 0.2520, Accuracy = 0.7500\n",
            "Epoch 600: Prediction [[0.48108379]\n",
            " [0.51588681]\n",
            " [0.5016014 ]\n",
            " [0.46159288]]\n",
            " MSE = 0.2518, Accuracy = 0.5000\n",
            "Epoch 700: Prediction [[0.48440067]\n",
            " [0.46592304]\n",
            " [0.51866938]\n",
            " [0.50520677]]\n",
            " MSE = 0.2517, Accuracy = 0.5000\n",
            "Epoch 800: Prediction [[0.46907545]\n",
            " [0.48657018]\n",
            " [0.5202773 ]\n",
            " [0.50759989]]\n",
            " MSE = 0.2516, Accuracy = 0.5000\n",
            "Epoch 900: Prediction [[0.50920356]\n",
            " [0.47146091]\n",
            " [0.48799509]\n",
            " [0.52112329]]\n",
            " MSE = 0.2515, Accuracy = 0.5000\n",
            "Epoch 1000: Prediction [[0.52146155]\n",
            " [0.47333425]\n",
            " [0.48892434]\n",
            " [0.51027881]]\n",
            " MSE = 0.2515, Accuracy = 0.5000\n",
            "Epoch 1100: Prediction [[0.48952922]\n",
            " [0.52146579]\n",
            " [0.51100488]\n",
            " [0.47487231]]\n",
            " MSE = 0.2514, Accuracy = 0.5000\n",
            "Epoch 1200: Prediction [[0.4899304 ]\n",
            " [0.47619895]\n",
            " [0.52125951]\n",
            " [0.51150843]]\n",
            " MSE = 0.2513, Accuracy = 0.5000\n",
            "Epoch 1300: Prediction [[0.51185569]\n",
            " [0.47738034]\n",
            " [0.49019124]\n",
            " [0.52090655]]\n",
            " MSE = 0.2512, Accuracy = 0.5000\n",
            "Epoch 1400: Prediction [[0.52048261]\n",
            " [0.49038595]\n",
            " [0.51212466]\n",
            " [0.4784936 ]]\n",
            " MSE = 0.2512, Accuracy = 0.5000\n",
            "Epoch 1500: Prediction [[0.51230143]\n",
            " [0.49050118]\n",
            " [0.47952471]\n",
            " [0.51997443]]\n",
            " MSE = 0.2511, Accuracy = 0.5000\n",
            "Epoch 1600: Prediction [[0.51941557]\n",
            " [0.48050953]\n",
            " [0.49057055]\n",
            " [0.5124211 ]]\n",
            " MSE = 0.2510, Accuracy = 0.5000\n",
            "Epoch 1700: Prediction [[0.5188524 ]\n",
            " [0.48149525]\n",
            " [0.49063927]\n",
            " [0.51253139]]\n",
            " MSE = 0.2509, Accuracy = 0.5000\n",
            "Epoch 1800: Prediction [[0.4906958 ]\n",
            " [0.51827265]\n",
            " [0.48247075]\n",
            " [0.51262004]]\n",
            " MSE = 0.2509, Accuracy = 0.5000\n",
            "Epoch 1900: Prediction [[0.51268851]\n",
            " [0.48343761]\n",
            " [0.51767763]\n",
            " [0.49074117]]\n",
            " MSE = 0.2508, Accuracy = 0.5000\n",
            "Epoch 2000: Prediction [[0.51706437]\n",
            " [0.49077228]\n",
            " [0.51273361]\n",
            " [0.48439275]]\n",
            " MSE = 0.2507, Accuracy = 0.5000\n",
            "Epoch 2100: Prediction [[0.51643803]\n",
            " [0.48534307]\n",
            " [0.51276117]\n",
            " [0.49079472]]\n",
            " MSE = 0.2506, Accuracy = 0.5000\n",
            "Epoch 2200: Prediction [[0.49081095]\n",
            " [0.51580176]\n",
            " [0.48629116]\n",
            " [0.51277424]]\n",
            " MSE = 0.2505, Accuracy = 0.5000\n",
            "Epoch 2300: Prediction [[0.49082523]\n",
            " [0.51277707]\n",
            " [0.48724305]\n",
            " [0.51515884]]\n",
            " MSE = 0.2505, Accuracy = 0.5000\n",
            "Epoch 2400: Prediction [[0.51453006]\n",
            " [0.51279026]\n",
            " [0.4882177 ]\n",
            " [0.49085645]]\n",
            " MSE = 0.2504, Accuracy = 0.5000\n",
            "Epoch 2500: Prediction [[0.51276616]\n",
            " [0.49085953]\n",
            " [0.48916904]\n",
            " [0.5138687 ]]\n",
            " MSE = 0.2503, Accuracy = 0.5000\n",
            "Epoch 2600: Prediction [[0.51273933]\n",
            " [0.4908671 ]\n",
            " [0.51320787]\n",
            " [0.49013214]]\n",
            " MSE = 0.2502, Accuracy = 0.5000\n",
            "Epoch 2700: Prediction [[0.51270914]\n",
            " [0.51254709]\n",
            " [0.49087821]\n",
            " [0.49110617]]\n",
            " MSE = 0.2501, Accuracy = 0.5000\n",
            "Epoch 2800: Prediction [[0.49089061]\n",
            " [0.5118839 ]\n",
            " [0.4920895 ]\n",
            " [0.51267331]]\n",
            " MSE = 0.2500, Accuracy = 0.5000\n",
            "Epoch 2900: Prediction [[0.51120698]\n",
            " [0.49089271]\n",
            " [0.51262   ]\n",
            " [0.49307009]]\n",
            " MSE = 0.2499, Accuracy = 0.5000\n",
            "Epoch 3000: Prediction [[0.49405876]\n",
            " [0.51255949]\n",
            " [0.51052592]\n",
            " [0.49089396]]\n",
            " MSE = 0.2498, Accuracy = 0.5000\n",
            "Epoch 3100: Prediction [[0.49092401]\n",
            " [0.51252316]\n",
            " [0.50987089]\n",
            " [0.49508734]]\n",
            " MSE = 0.2497, Accuracy = 0.5000\n",
            "Epoch 3200: Prediction [[0.49093807]\n",
            " [0.50919645]\n",
            " [0.51246403]\n",
            " [0.49610906]]\n",
            " MSE = 0.2496, Accuracy = 0.5000\n",
            "Epoch 3300: Prediction [[0.49096369]\n",
            " [0.50853089]\n",
            " [0.51241144]\n",
            " [0.49715337]]\n",
            " MSE = 0.2495, Accuracy = 0.5000\n",
            "Epoch 3400: Prediction [[0.49097475]\n",
            " [0.51233789]\n",
            " [0.50784732]\n",
            " [0.49819359]]\n",
            " MSE = 0.2494, Accuracy = 0.5000\n",
            "Epoch 3500: Prediction [[0.51223565]\n",
            " [0.49922159]\n",
            " [0.49096373]\n",
            " [0.50713861]]\n",
            " MSE = 0.2493, Accuracy = 0.5000\n",
            "Epoch 3600: Prediction [[0.5121235 ]\n",
            " [0.50025694]\n",
            " [0.50642228]\n",
            " [0.49094799]]\n",
            " MSE = 0.2492, Accuracy = 0.7500\n",
            "Epoch 3700: Prediction [[0.5013057 ]\n",
            " [0.51200761]\n",
            " [0.5057045 ]\n",
            " [0.49093317]]\n",
            " MSE = 0.2490, Accuracy = 0.7500\n",
            "Epoch 3800: Prediction [[0.50237661]\n",
            " [0.49092701]\n",
            " [0.51189647]\n",
            " [0.50499329]]\n",
            " MSE = 0.2489, Accuracy = 0.7500\n",
            "Epoch 3900: Prediction [[0.49092337]\n",
            " [0.5117837 ]\n",
            " [0.50346378]\n",
            " [0.50428237]]\n",
            " MSE = 0.2488, Accuracy = 0.7500\n",
            "Epoch 4000: Prediction [[0.5116472 ]\n",
            " [0.49090082]\n",
            " [0.50355066]\n",
            " [0.50454439]]\n",
            " MSE = 0.2487, Accuracy = 0.7500\n",
            "Epoch 4100: Prediction [[0.51149962]\n",
            " [0.49087147]\n",
            " [0.50281029]\n",
            " [0.50563176]]\n",
            " MSE = 0.2485, Accuracy = 0.7500\n",
            "Epoch 4200: Prediction [[0.50206181]\n",
            " [0.50672746]\n",
            " [0.51134199]\n",
            " [0.49083582]]\n",
            " MSE = 0.2484, Accuracy = 0.7500\n",
            "Epoch 4300: Prediction [[0.49080034]\n",
            " [0.50783852]\n",
            " [0.51118129]\n",
            " [0.50131204]]\n",
            " MSE = 0.2483, Accuracy = 0.7500\n",
            "Epoch 4400: Prediction [[0.51102568]\n",
            " [0.49077257]\n",
            " [0.5089738 ]\n",
            " [0.50056855]]\n",
            " MSE = 0.2481, Accuracy = 0.7500\n",
            "Epoch 4500: Prediction [[0.5108445 ]\n",
            " [0.49072316]\n",
            " [0.51010183]\n",
            " [0.49980218]]\n",
            " MSE = 0.2480, Accuracy = 0.5000\n",
            "Epoch 4600: Prediction [[0.49903916]\n",
            " [0.49067771]\n",
            " [0.51066517]\n",
            " [0.51125035]]\n",
            " MSE = 0.2478, Accuracy = 0.5000\n",
            "Epoch 4700: Prediction [[0.49828052]\n",
            " [0.49063684]\n",
            " [0.51048849]\n",
            " [0.51242008]]\n",
            " MSE = 0.2477, Accuracy = 0.5000\n",
            "Epoch 4800: Prediction [[0.49057048]\n",
            " [0.51357994]\n",
            " [0.5102828 ]\n",
            " [0.49749531]]\n",
            " MSE = 0.2475, Accuracy = 0.5000\n",
            "Epoch 4900: Prediction [[0.51476923]\n",
            " [0.49051539]\n",
            " [0.51008758]\n",
            " [0.49672168]]\n",
            " MSE = 0.2474, Accuracy = 0.5000\n",
            "Epoch 5000: Prediction [[0.50988336]\n",
            " [0.51596881]\n",
            " [0.4904534 ]\n",
            " [0.49594096]]\n",
            " MSE = 0.2472, Accuracy = 0.5000\n",
            "Epoch 5100: Prediction [[0.49516253]\n",
            " [0.50968013]\n",
            " [0.49039346]\n",
            " [0.51718887]]\n",
            " MSE = 0.2471, Accuracy = 0.5000\n",
            "Epoch 5200: Prediction [[0.50946788]\n",
            " [0.51841872]\n",
            " [0.49032568]\n",
            " [0.494377  ]]\n",
            " MSE = 0.2469, Accuracy = 0.5000\n",
            "Epoch 5300: Prediction [[0.49356288]\n",
            " [0.51963627]\n",
            " [0.50922416]\n",
            " [0.49022917]]\n",
            " MSE = 0.2467, Accuracy = 0.5000\n",
            "Epoch 5400: Prediction [[0.49272995]\n",
            " [0.52085187]\n",
            " [0.50895921]\n",
            " [0.49011326]]\n",
            " MSE = 0.2465, Accuracy = 0.5000\n",
            "Epoch 5500: Prediction [[0.52209551]\n",
            " [0.49190609]\n",
            " [0.50870243]\n",
            " [0.49000523]]\n",
            " MSE = 0.2464, Accuracy = 0.5000\n",
            "Epoch 5600: Prediction [[0.52331954]\n",
            " [0.49104793]\n",
            " [0.50840775]\n",
            " [0.48986161]]\n",
            " MSE = 0.2462, Accuracy = 0.5000\n",
            "Epoch 5700: Prediction [[0.48973706]\n",
            " [0.50813372]\n",
            " [0.52458409]\n",
            " [0.49021085]]\n",
            " MSE = 0.2460, Accuracy = 0.5000\n",
            "Epoch 5800: Prediction [[0.48938936]\n",
            " [0.52588286]\n",
            " [0.48962588]\n",
            " [0.50787429]]\n",
            " MSE = 0.2458, Accuracy = 0.5000\n",
            "Epoch 5900: Prediction [[0.52719204]\n",
            " [0.48856022]\n",
            " [0.4895053 ]\n",
            " [0.50760549]]\n",
            " MSE = 0.2456, Accuracy = 0.5000\n",
            "Epoch 6000: Prediction [[0.487735  ]\n",
            " [0.48938557]\n",
            " [0.528522  ]\n",
            " [0.50733886]]\n",
            " MSE = 0.2454, Accuracy = 0.5000\n",
            "Epoch 6100: Prediction [[0.48925088]\n",
            " [0.5070573 ]\n",
            " [0.52985615]\n",
            " [0.48689722]]\n",
            " MSE = 0.2452, Accuracy = 0.5000\n",
            "Epoch 6200: Prediction [[0.50675394]\n",
            " [0.53118752]\n",
            " [0.48604041]\n",
            " [0.48909462]]\n",
            " MSE = 0.2450, Accuracy = 0.5000\n",
            "Epoch 6300: Prediction [[0.48891689]\n",
            " [0.48516498]\n",
            " [0.53251619]\n",
            " [0.5064291 ]]\n",
            " MSE = 0.2448, Accuracy = 0.5000\n",
            "Epoch 6400: Prediction [[0.53386179]\n",
            " [0.5061018 ]\n",
            " [0.48873528]\n",
            " [0.48428885]]\n",
            " MSE = 0.2446, Accuracy = 0.5000\n",
            "Epoch 6500: Prediction [[0.50577519]\n",
            " [0.48341544]\n",
            " [0.53522625]\n",
            " [0.48855213]]\n",
            " MSE = 0.2443, Accuracy = 0.5000\n",
            "Epoch 6600: Prediction [[0.50545266]\n",
            " [0.53661396]\n",
            " [0.48254767]\n",
            " [0.48837075]]\n",
            " MSE = 0.2441, Accuracy = 0.5000\n",
            "Epoch 6700: Prediction [[0.50511193]\n",
            " [0.48166453]\n",
            " [0.48817028]\n",
            " [0.53800211]]\n",
            " MSE = 0.2439, Accuracy = 0.5000\n",
            "Epoch 6800: Prediction [[0.50476832]\n",
            " [0.48796456]\n",
            " [0.5394057 ]\n",
            " [0.48078068]]\n",
            " MSE = 0.2437, Accuracy = 0.5000\n",
            "Epoch 6900: Prediction [[0.47991928]\n",
            " [0.5044459 ]\n",
            " [0.54084842]\n",
            " [0.48777574]]\n",
            " MSE = 0.2434, Accuracy = 0.5000\n",
            "Epoch 7000: Prediction [[0.47901922]\n",
            " [0.4875441 ]\n",
            " [0.50408055]\n",
            " [0.54226587]]\n",
            " MSE = 0.2432, Accuracy = 0.5000\n",
            "Epoch 7100: Prediction [[0.47812541]\n",
            " [0.50371952]\n",
            " [0.54370579]\n",
            " [0.48731347]]\n",
            " MSE = 0.2429, Accuracy = 0.5000\n",
            "Epoch 7200: Prediction [[0.48705176]\n",
            " [0.54513202]\n",
            " [0.47720619]\n",
            " [0.5033287 ]]\n",
            " MSE = 0.2427, Accuracy = 0.5000\n",
            "Epoch 7300: Prediction [[0.54657907]\n",
            " [0.48678946]\n",
            " [0.50294105]\n",
            " [0.47629217]]\n",
            " MSE = 0.2424, Accuracy = 0.5000\n",
            "Epoch 7400: Prediction [[0.54803018]\n",
            " [0.48651172]\n",
            " [0.50254048]\n",
            " [0.47536851]]\n",
            " MSE = 0.2422, Accuracy = 0.5000\n",
            "Epoch 7500: Prediction [[0.50214766]\n",
            " [0.4744549 ]\n",
            " [0.48623713]\n",
            " [0.54950538]]\n",
            " MSE = 0.2419, Accuracy = 0.5000\n",
            "Epoch 7600: Prediction [[0.55099256]\n",
            " [0.50174963]\n",
            " [0.47353895]\n",
            " [0.4859542 ]]\n",
            " MSE = 0.2417, Accuracy = 0.5000\n",
            "Epoch 7700: Prediction [[0.47261199]\n",
            " [0.55248092]\n",
            " [0.48565341]\n",
            " [0.50133705]]\n",
            " MSE = 0.2414, Accuracy = 0.5000\n",
            "Epoch 7800: Prediction [[0.50094325]\n",
            " [0.55400355]\n",
            " [0.47170576]\n",
            " [0.48536539]]\n",
            " MSE = 0.2411, Accuracy = 0.5000\n",
            "Epoch 7900: Prediction [[0.47077768]\n",
            " [0.50052321]\n",
            " [0.48504863]\n",
            " [0.55551517]]\n",
            " MSE = 0.2409, Accuracy = 0.5000\n",
            "Epoch 8000: Prediction [[0.50010158]\n",
            " [0.4847257 ]\n",
            " [0.55704031]\n",
            " [0.46985115]]\n",
            " MSE = 0.2406, Accuracy = 0.5000\n",
            "Epoch 8100: Prediction [[0.46889689]\n",
            " [0.48436772]\n",
            " [0.49964731]\n",
            " [0.55854727]]\n",
            " MSE = 0.2403, Accuracy = 0.7500\n",
            "Epoch 8200: Prediction [[0.49918218]\n",
            " [0.48399446]\n",
            " [0.46793542]\n",
            " [0.56005771]]\n",
            " MSE = 0.2400, Accuracy = 0.7500\n",
            "Epoch 8300: Prediction [[0.49874046]\n",
            " [0.5616057 ]\n",
            " [0.46699951]\n",
            " [0.48363777]]\n",
            " MSE = 0.2397, Accuracy = 0.7500\n",
            "Epoch 8400: Prediction [[0.48328468]\n",
            " [0.49830886]\n",
            " [0.4660766 ]\n",
            " [0.56317688]]\n",
            " MSE = 0.2394, Accuracy = 0.7500\n",
            "Epoch 8500: Prediction [[0.4651467 ]\n",
            " [0.48291605]\n",
            " [0.56475034]\n",
            " [0.49786616]]\n",
            " MSE = 0.2391, Accuracy = 0.7500\n",
            "Epoch 8600: Prediction [[0.46421767]\n",
            " [0.48253899]\n",
            " [0.4974208 ]\n",
            " [0.56633385]]\n",
            " MSE = 0.2389, Accuracy = 0.7500\n",
            "Epoch 8700: Prediction [[0.48213282]\n",
            " [0.56790436]\n",
            " [0.46326816]\n",
            " [0.49694984]]\n",
            " MSE = 0.2385, Accuracy = 0.7500\n",
            "Epoch 8800: Prediction [[0.48171376]\n",
            " [0.46231572]\n",
            " [0.4964718 ]\n",
            " [0.56947917]]\n",
            " MSE = 0.2382, Accuracy = 0.7500\n",
            "Epoch 8900: Prediction [[0.4812794 ]\n",
            " [0.46135765]\n",
            " [0.49598369]\n",
            " [0.5710554 ]]\n",
            " MSE = 0.2379, Accuracy = 0.7500\n",
            "Epoch 9000: Prediction [[0.57263852]\n",
            " [0.49549159]\n",
            " [0.46039973]\n",
            " [0.48083497]]\n",
            " MSE = 0.2376, Accuracy = 0.7500\n",
            "Epoch 9100: Prediction [[0.4803957 ]\n",
            " [0.57424493]\n",
            " [0.49501262]\n",
            " [0.45945805]]\n",
            " MSE = 0.2373, Accuracy = 0.7500\n",
            "Epoch 9200: Prediction [[0.45851344]\n",
            " [0.4799435 ]\n",
            " [0.49452658]\n",
            " [0.57585522]]\n",
            " MSE = 0.2370, Accuracy = 0.7500\n",
            "Epoch 9300: Prediction [[0.45755462]\n",
            " [0.47946624]\n",
            " [0.49402072]\n",
            " [0.57745452]]\n",
            " MSE = 0.2367, Accuracy = 0.7500\n",
            "Epoch 9400: Prediction [[0.47898456]\n",
            " [0.45660279]\n",
            " [0.49351799]\n",
            " [0.57906582]]\n",
            " MSE = 0.2364, Accuracy = 0.7500\n",
            "Epoch 9500: Prediction [[0.47849925]\n",
            " [0.45565895]\n",
            " [0.49301927]\n",
            " [0.58068948]]\n",
            " MSE = 0.2360, Accuracy = 0.7500\n",
            "Epoch 9600: Prediction [[0.45472431]\n",
            " [0.47801151]\n",
            " [0.58232667]\n",
            " [0.49252583]]\n",
            " MSE = 0.2357, Accuracy = 0.7500\n",
            "Epoch 9700: Prediction [[0.47748304]\n",
            " [0.49199572]\n",
            " [0.45375978]\n",
            " [0.5839348 ]]\n",
            " MSE = 0.2354, Accuracy = 0.7500\n",
            "Epoch 9800: Prediction [[0.585572  ]\n",
            " [0.49148714]\n",
            " [0.45281976]\n",
            " [0.47696683]]\n",
            " MSE = 0.2351, Accuracy = 0.7500\n",
            "Epoch 9900: Prediction [[0.49094463]\n",
            " [0.47641151]\n",
            " [0.58718075]\n",
            " [0.45185267]]\n",
            " MSE = 0.2347, Accuracy = 0.7500\n"
          ]
        }
      ],
      "source": [
        "nnet.train(X_data, y_data, learning_rate=0.01, num_epochs=10000, verbose=True, randomize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "id": "wxA9Y65bYu7g"
      },
      "source": [
        "## Print the resuting neural net weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 861,
      "metadata": {
        "collapsed": true,
        "nbgrader": {
          "grade": false,
          "locked": true,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuFj3u1gYu7g",
        "outputId": "4b8b224b-43bb-4690-a631-f4c03f702f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: [Input] shape 2, use_bias=False, activation=None\n",
            "weights: None\n",
            "\n",
            "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
            "weights: [[ 1.00997277 -2.06861199]\n",
            " [ 0.52874482  1.53818065]]\n",
            "bias: [ 0.7707856  -1.22763539]\n",
            "\n",
            "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n",
            "weights: [[0.6491146 ]\n",
            " [1.11102788]]\n",
            "bias: [-0.79220917]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nnet.summary(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "jupyter nbconvert --to html /content/lab02.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrhOKT6DADRe",
        "outputId": "0675fca3-194c-4879-867e-b7cb11d766f5"
      },
      "execution_count": 835,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/lab02.ipynb to html\n",
            "[NbConvertApp] Writing 654355 bytes to /content/lab02.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 835
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add a text block to your Notebook: Explain in 2-3 sentences what the network structure is, its purpose and how it works (overview).\n",
        "\n",
        "Additionally, in a few bullets or additional short paragraph: Explain your results clearly in your own words, discussing the accuracy of the model and what the results mean for answering the machine learning problem being posed."
      ],
      "metadata": {
        "id": "gMRb5cRCO__D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "1. The model structure is pretty simple. It looks like 2 input neurons, 2 dense neurons and 1 output neuron. It works by taking the input and feeding it forward, multiplying weight by input adding bias, done by utilizing the predict function (which calls feed_forward), then backpropagating on the output layer, then looping through the dense layers back to the input to adjust the weights by calculating the delta/gradient and applying the error, before passing that error backwards to the next layer.\n",
        "2. The accuracy slowly increases from 50% to 75% before plateauing based on the input across the epochs. If I left it to a smaller number of epochs we do not reach this number.\n",
        "3. I had a lot of issues finagling with the indices and matrix conversions and at one point was unsure of which way I was iterating through the list of layers so that might have had an impact on my algorithms and code structure. I tried to correct and go back through my implementation to ensure I was iterating properly - at one point I was calculating error twice for different layers and applying it twice.\n",
        "4. One thing that I was reading on supplemental materials and it isn't here (at least in my implementation) is that the error should be calculated across all layers before updating the weights? I think there is a different approach here where we calculate the error and backpropagate it to each layer with a cascading effect."
      ],
      "metadata": {
        "id": "QEcuOVwzOdFI"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}